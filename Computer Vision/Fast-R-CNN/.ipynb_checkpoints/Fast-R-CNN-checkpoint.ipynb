{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "Object Detection is more difficult than image classification\n",
    "\n",
    "Because detection needs the accurate locaiton of objects. Which brings two major difficulties:\n",
    "1. numerous candidate object locations (often called “proposals”) must be processed.\n",
    "2. these candidates provide only rough localization that must be refined to achieve precise localization.\n",
    "\n",
    "\n",
    "Previouse approches use **multi-stage pipline** ( Proposal-CNN-SVM-boundingBoxRegression) \n",
    "which is **slow** ( VOC07 2.5GPU days, hundreds of gigabytes training and VGG16 takes 47s / image on a GPU) \n",
    "and inelegant.\n",
    "Fast-R-CNN using a single-stage training algorithm that jointly learns to classify object proposals and refine their spatial locations.\n",
    "\n",
    "\n",
    "R-CNN is slow because it performs a ConvNet forward pass for each object proposal, without sharing computation.\n",
    "\n",
    "Spatial pyramid pooling networks (SPPnets) were proposed to speed up R-CNN by sharing computation. SPPnet accelerates \n",
    "R-CNN by 10 to 100$\\times$ at test time. Training time is also reduced by 3$\\times$ due to faster proposal feature extraction.\n",
    "\n",
    "But SPPnets also use multi-stage pipeline, which stores all the fetures on disk.\n",
    "\n",
    "\n",
    "### Contributions\n",
    ">We propose a new training algorithm that fixes the disadvantages\n",
    ">of R-CNN and SPPnet, while improving on their\n",
    ">speed and accuracy. We call this method Fast R-CNN because\n",
    ">it’s comparatively fast to train and test. The Fast RCNN\n",
    ">method has several advantages:\n",
    "1. Higher detection quality (mAP) than R-CNN, SPPnet\n",
    "2. Training is single-stage, using a multi-task loss\n",
    "3. Training can update all network layers\n",
    "4. No disk storage is required for feature caching\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall structure\n",
    "![](pic1.png)\n",
    "![](diagram1.png)\n",
    "\n",
    "- The feature map won't be fixed size due to the size variation of input images. \n",
    "\n",
    "```\n",
    "graph LR\n",
    "subgraph \n",
    "A[Image] -->|Deep ConvNet| B[Feature map]\n",
    "C[proposals/RoIs]-->|Input| D((Extractor))\n",
    "B-->|Input|D\n",
    "D--> E[sub-Features for each RoI]\n",
    "end\n",
    "subgraph\n",
    "E-->F(RoI pooling layers) \n",
    "F-->G(FC layers)\n",
    "\n",
    "G-->|FC|H[label]\n",
    "G-->|FC|I[position]\n",
    "end\n",
    "```\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoI pooling layer \n",
    " #### Purpose: \n",
    " Convert the fetures in side any RoI to a fixed size feature map.\n",
    " #### Input\n",
    " Features inside RoI(can be in any shape)\n",
    " #### Output\n",
    " Fixed shaped features\n",
    " #### Details\n",
    " Using max pooling, pooling indepedent for each channel.\n",
    " Is a special case of the spatial pyramid pooling layer used in\n",
    " SPPnets. \n",
    "\n",
    " For more detailed sub-window calculation see: ***K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\n",
    " in deep convolutional networks for visual recognition. In\n",
    " ECCV, 2014***\n",
    "\n",
    "https://github.com/deepsense-ai/roi-pooling\n",
    "----\n",
    "### Initializing with pretrained networks\n",
    " \n",
    " #### Three steps to modify a pretrained ImageNet network to Fast-R-CNN\n",
    " 1. replace last max pooling layer by RoI pooling layer\n",
    " 2. >The network’s last fully connected layer and softmax(which were trained for 1000-way ImageNet classification) are replaced with the two sibling layers described earlier (a fully connected layer and softmax over K+1 categoriesand  **category-specific** bounding-box regressors).\n",
    " 3. Third, the network is modified to take two data inputs: a list of images and a list of RoIs in those images.\n",
    "\n",
    "----\n",
    "### Fine tuning for detection\n",
    "\n",
    "#### why SPPnet is unable to update weights below the spatial pyramid pooling layer\n",
    " 1. Back-propagation through the SPP is highly inefficient when each training sample(RoI) comes from a different image.\n",
    " 2. The inefficiency stems from the fact that each RoI may have a very large receptive field, often spanning the entire input image.\n",
    "\n",
    "#### Proposed method\n",
    "##### main idea\n",
    " takes advantage of feature sharing during training\n",
    "##### Details\n",
    " 1. Hierarchical sample, sample N images and then sample R/N RoI in each image. (N=2, R=128 are used in the paper)\n",
    " 2. Jointly optimizes a softmax classifier and bounding-box regressors\n",
    "\n",
    "----\n",
    "### Multi task loss\n",
    "$$\n",
    "  L(p,u,t^u,v) = L_{cls}(p,u) + \\lambda[u\\ge1]L_{loc}(t^v,v)\n",
    "$$\n",
    "\n",
    "$p=(p_0,p_1,...,p_K)$ is the probability distribution over K+1 categories.\n",
    "\n",
    "$t^k = (t_x^k,t_y^k,t_w^k,t_h^k)$ is the output of bbox regression, for each of the K categories indexed by k.\n",
    "\n",
    "$u$ is the ground-truth class.\n",
    "\n",
    "$v = (v_x,v_y,v_w,v_h)$ is the ground truth bbox for class u. \n",
    "\n",
    "$L_{cls}(p,u) = -log(p_u)$ is log loss for class $u$.\n",
    "\n",
    "$\n",
    "[u\\ge1] = \\begin{cases}\n",
    "0 & u\\lt1 \\\\\n",
    "1 & otherwise\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "$\n",
    "L_{loc}(t^u,v)=\\Sigma_{i\\in \\{x,y,w,h\\}}{smooth_{L1}(t_i^u-vi)}\n",
    "$\n",
    "\n",
    "$\n",
    "smooth_{L1}(x) = \\begin{cases}\n",
    "0.5x^2 & if\\ |x|<1 \\\\\n",
    "|x|-0.5 & otherwise\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "This paper claims robust $L_1$ norm is less sensitive to outliers than $L_2$ loss.\n",
    "Which makes tuning the network easier.\n",
    "\n",
    "---\n",
    "\n",
    "### Mini-batch sampling.\n",
    "How to choose training samples?\n",
    "1. Each mini-batch constructed from N=2 image, each image have 64 RoI in it. \n",
    "2. Choose 25% of the samples from proposals that have $ IoU \\ge 0.5$.\n",
    "3. Choose 75% of the samples from proposals that have $ IoU \\in [0.1,0.5)$ and label them as 0(back ground).\n",
    "\n",
    "> During training, images are horizontally flipped with\n",
    "probability 0:5. No other data augmentation is used\n",
    "\n",
    "---\n",
    "### Back propagation through RoI pooling layers\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = \\underset{r}{\\Sigma}{\\underset{j}{\\Sigma}{[i=i\\ast (r,j)]\\frac{\\partial L}{\\partial y_{rj}}}}\n",
    "$$\n",
    "\n",
    "$ x_i \\in \\Re$ is the $i$-th input activation to the RoI pooling layer.\n",
    "\n",
    "$ y_{rj} = x_{i\\ast(r,j)} $ is the output of the pooling layer \n",
    "\n",
    "$ i\\ast (r,j) = argmax_{{i}^{'} \\in R(r,j)} $ \n",
    "\n",
    "$ R(r,j)$ is the index set of inputs in the sub-window over which the the output unit $y_{r,j}$ pools.\n",
    "\n",
    ">for each mini-batch RoI r and for each pooling\n",
    "output unit $y_{rj}$ , the partial derivative $\\partial L/\\partial y_{rj}$ is accumulated\n",
    "if i is the argmax selected for $y_{rj}$ by max pooling\n",
    "\n",
    "---\n",
    "### SGD hyper-parameters\n",
    "**initialization** \n",
    "fc layer and bbox regressor: zero-mean Gaussian distributions with standard deviations 0:01 and 0:001, bias = 0.\n",
    "\n",
    "**learning rate**\n",
    "per-layer learning rate: 1 for weights 2 for bias.\n",
    "global learning rate 0.001\n",
    "\n",
    "learning rate decay: \n",
    "   - VOC07, VOC12 : 30K iteration then 10K with 0.0001.\n",
    "   - larger dataset: A momentum of 0.9 and parameter decay of 0.0005 (on weights and biases) are used.\n",
    "    \n",
    "    \n",
    "--- \n",
    "### Scale invariance\n",
    "** Two Approaches**\n",
    "1. Brute force: Input fixed size images and the network need to learn the scale invariant detection.\n",
    "2. Using image pyramids: \n",
    "    + data augmentation during training.\n",
    "    + it's a approximation of scale invariance.\n",
    "\n",
    "more details:\n",
    "***K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\n",
    " in deep convolutional networks for visual recognition. In\n",
    " ECCV, 2014***\n",
    " \n",
    " --- \n",
    "### Fast-R-CNN detection.\n",
    "After the network is done trianing. \n",
    "The input of the network should be:\n",
    "   1. image or image pyramid encoded as a list of images\n",
    "   2. a list of RoI which is pre-computed.\n",
    " \n",
    " \n",
    "The output of the network should be:\n",
    "   1. For each test RoI $r$, the forward pass outputs a class posterior probability distribution $p$.\n",
    "   2. a set of predicted bounding-box offsets relative to $r$.\n",
    "\n",
    "After the network output there is a extra operation:\n",
    ">We then perform non-maximum suppression independently for each\n",
    "class using the algorithm and settings from R-CN\n",
    "***R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature\n",
    "hierarchies for accurate object detection and semantic\n",
    "segmentation. In CVPR, 2014***\n",
    "\n",
    "___\n",
    "\n",
    "### Using truncated SVD to speedup  detection\n",
    "**Scenario to use**: Object detection.\n",
    "In this scenario(different from classification), because of the large number of RoIs, network spend near half of its time computing fc layers.\n",
    "\n",
    "**Technique**: truncated SVD\n",
    "Basic idea: split one single fc layer into 2 layers, reducing the total amount of parameters.\n",
    "\n",
    "**Input**: fc layer $ W, b$\n",
    "\n",
    "**Output**: two conssecutive fc layers $W_1, b_1, W_2, b_2$, and **No none-linearity between them**\n",
    "\n",
    "**Process**:\n",
    " 1. perform SVD on $W$, $W = U_0\\Sigma V_0^T$\n",
    " 2. Choose first $t$ singular value to approximate $W$, $ W \\approx U\\Sigma_t V^T$\n",
    " 3 $W_1 = \\Sigma_tV^T$, $b_1 = 0$ \n",
    " 4 $W_2 = U$, $b_2 = b$\n",
    " \n",
    "**Analysis**:\n",
    " \n",
    "  If $W$ is $u-by-v$ matrix, then $U$ is a $u-by-t$ matrix and $\\Sigma_tV^T$ is a $t-by-v$ matrix.\n",
    " \n",
    "  So, the amount of parameters is reduced from $uv$ to $t(u+v)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Which layers to finetune?\n",
    "1. If only fine-tuning the fc layer, the mAP decreased from 66.9% to 61.4%. Which means that the back-propogation through RoI pooling is important for deep networks.\n",
    "2. It only necessary to update layers from conv3 1 and up (9 of the 13 conv layers).\n",
    "\n",
    " Two reasons: \n",
    "  1. updating from conv2_1 slows training by $1.3\\times$ (12.5 vs. 9.5 hours) compared to learning from conv3_1\n",
    "  2. updating from conv1_1 **over-runs GPU memory**.\n",
    "  3. The difference in mAP when learning from conv2_1 up was only +0.3 points\n",
    "\n",
    "\n",
    "##### My thought: \n",
    "\n",
    "It's possible to make model small if we fix the botton layer of CNN?\n",
    "\n",
    "---\n",
    "\n",
    "### Scale invariance: to brute force or finesse?\n",
    "\n",
    "|     |  SPPnet ZF |   S    |   M   |   L   |\n",
    "| ---- |:-----------:|-------|------|------|\n",
    "|scales|1   /   5|1   /  5|1  /  5|1 |\n",
    "|test rate(s/im)|0.14 / 0.38|0.10 / 0.39|0.15 / 0.64|0.32|\n",
    "|VOC07 mAP|58.0 / 59.2|57.1 / 58.4|59.2 / 60.7|66.9|\n",
    "\n",
    "The result of multi-scale and single scale shows that multi-scale provide slightly higher mAP.\n",
    "But it also takes much more time to compute.\n",
    "So, the single scale is the best trade-off between accuracy and speed.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Do SVMs outperform softmax?\n",
    "Anwser is **NO**.\n",
    ">softmax slightly outperforming SVM for all three networks, by +0:1 to +0:8 mAP point\n",
    "\n",
    "**Thoughts**\n",
    "At first I was thinking compare SVMs to fc layers. Why compare SVM to softmax?\n",
    "\n",
    "### Are more proposals always better?\n",
    "> We find evidence that the proposalclassifier\n",
    "cascade also improves Fast R-CNN accuracy\n",
    "\n",
    ">Classifying sparse proposals is a type of cascade [22] in\n",
    "which the proposal mechanism first rejects a vast number of\n",
    "candidates leaving the classifier with a small set to evaluate\n",
    "\n",
    "So there are brodly two types of proposal methods\n",
    "1. sparse set of object proposals\n",
    "2. dense set of proposals\n",
    "\n",
    "This paper find out that sparse proposal method improves Fast-R-CNN's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mAP: Mean average precision\n",
    "The score for Information retrieval\n",
    "$$ MAP = \\frac{\\Sigma^{Q}_{q=1}{avgP(q)}}{Q} $$\n",
    "Q is the number of queries.\n",
    "\n",
    "\n",
    "- intersection over union (IoU)\n",
    "$$\n",
    "    IoU = \\frac{DetectionResult \\cap GroundTruth}{DetectionResult\\cup GroundTruth}\n",
    "$$\n",
    "\n",
    "\n",
    "- Hard negative mining\n",
    "\n",
    "http://cs.brown.edu/courses/cs143/2011/results/proj4/psastras/\n",
    "\n",
    ">During the first training stage positive crops are used from the training data coupled with negative training crops randomly chosen from images without a face. To refine the SVM, the detector is run again on the non face scenes, and any detections (these are false positives) are used as new negative training examples to train another SVM, to decrease the false positive rate. This step can be repeated multiple times.\n",
    "\n",
    ">The strategy used for mining hard negatives was relatively simple: for each non face scene image, any detected faces were sorted by confidence, with the top results used as hard negative examples. Hard negative mining usually improved performance by about 5-10%, but seemed much more effective for the RBF SVM.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
