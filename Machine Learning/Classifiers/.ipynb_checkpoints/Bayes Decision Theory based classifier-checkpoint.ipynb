{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes of ***pattern recognition Sergios/Konstantinos 4th edition*** \n",
    "\n",
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Classification task:\n",
    " Given $M$ classes $w_1, w_2, w_3,..., w_M$ and the unknow pattern which is represented by feature vector $x$.\n",
    " \n",
    " We from the M conditional probabilities $P(w_i|x),i=1,2,3,..., M$. Sometimes these are also referred as **a posterior probabilities**.\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes decision theory\n",
    "\n",
    "#### Two classes case:\n",
    " \n",
    "  Let $w_1, w_2$ be the two classes. \n",
    " \n",
    "  **Assumptions**:\n",
    "  1. we assume the **a priori probabilities**  $P(w_1), P(w_2)$are known. This assumption is reasonable because we can easily estimate these two parameters from training dataset.\n",
    "  2. assume we know the class-conditional probability dense functions $p(x|w_i)$. The pdf $p(x|w_i)$ sometimes referred to as **likelihood function**\n",
    "  3. assume $p(x|w_i)$ denote the density function for continue variable $x$, $P(x|w_i)$ denotes the discrete case.\n",
    "  \n",
    "  \n",
    "  **Bayes Rule**:\n",
    "  \n",
    "  $$ P(w_i|x) = \\frac{p(x|w_i)P(w_i)}{p(x)}$$\n",
    "  \n",
    "  where $p(x)$ is the pdf of x and for which we have:\n",
    "  \n",
    "  $$ p(x) = \\overset{2}{\\underset{i=1}{\\Sigma}}{p(x|w_i)P(w_i)}$$\n",
    "  \n",
    "  ***Bayes classification rule***\n",
    "  \n",
    "  If $P(w_1|x) \\gt P(w_2|x)$ $x$ is classified to $w_1$.\n",
    "  \n",
    "  If $P(w_1|x) \\lt P(w_2|x)$ $x$ is classified to $w_2$.\n",
    "  \n",
    "  \n",
    "  ***Simplified classification***\n",
    "  When compare $P(w_i|x)$ for each $i$, the $p(x)$ stays the same. So we only need to compare:\n",
    "  \n",
    "  $$ p(x|w_i)P(w_1) \\lessgtr p(x|w_2)P(w_2)$$\n",
    "  \n",
    "  **Summary**\n",
    "      \n",
    "   1. The problem we want to solve: given feature vector $x$ what is the class $w_i$ it belongs to?\n",
    "   2. Using bayes rule, we need to know two things: \n",
    "        + ***a priori probabilities*** for each class.\n",
    "        + ***likelihood function*** $p(x|w_i)$ for each class with respect to $x$.\n",
    "   3. At last, we compare the joint probability of $x$ and $w_i$ instead of conditional probability.\n",
    "   \n",
    "   ---\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
