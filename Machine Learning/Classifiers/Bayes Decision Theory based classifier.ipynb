{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes of ***pattern recognition Sergios/Konstantinos 4th edition*** \n",
    "\n",
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Classification task:\n",
    " Given $M$ classes $w_1, w_2, w_3,..., w_M$ and the unknow pattern which is represented by feature vector $x$.\n",
    " \n",
    " We from the M conditional probabilities $P(w_i|x),i=1,2,3,..., M$. Sometimes these are also referred as **a posterior probabilities**.\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes decision theory\n",
    "\n",
    "#### Two classes case:\n",
    " \n",
    "  Let $w_1, w_2$ be the two classes. \n",
    " \n",
    "  **Assumptions**:\n",
    "  1. we assume the **a priori probabilities**  $P(w_1), P(w_2)$are known. This assumption is reasonable because we can easily estimate these two parameters from training dataset.\n",
    "  2. assume we know the class-conditional probability dense functions $p(x|w_i)$. The pdf $p(x|w_i)$ sometimes referred to as **likelihood function**\n",
    "  3. assume $p(x|w_i)$ denote the density function for continue variable $x$, $P(x|w_i)$ denotes the discrete case.\n",
    "  \n",
    "  \n",
    "  **Bayes Rule**:\n",
    "  \n",
    "  $$ P(w_i|x) = \\frac{p(x|w_i)P(w_i)}{p(x)}$$\n",
    "  \n",
    "  where $p(x)$ is the pdf of x and for which we have:\n",
    "  \n",
    "  $$ p(x) = \\overset{2}{\\underset{i=1}{\\Sigma}}{p(x|w_i)P(w_i)}$$\n",
    "  \n",
    "  ***Bayes classification rule***\n",
    "  \n",
    "  If $P(w_1|x) \\gt P(w_2|x)$ $x$ is classified to $w_1$.\n",
    "  \n",
    "  If $P(w_1|x) \\lt P(w_2|x)$ $x$ is classified to $w_2$.\n",
    "  \n",
    "  \n",
    "  ***Simplified classification***\n",
    "  When compare $P(w_i|x)$ for each $i$, the $p(x)$ stays the same. So we only need to compare:\n",
    "  \n",
    "  $$ p(x|w_i)P(w_1) \\lessgtr p(x|w_2)P(w_2)$$\n",
    "  \n",
    "  **Summary**\n",
    "      \n",
    "   1. The problem we want to solve: given feature vector $x$ what is the class $w_i$ it belongs to?\n",
    "   2. Using bayes rule, we need to know two things: \n",
    "        + ***a priori probabilities*** for each class.\n",
    "        + ***likelihood function*** $p(x|w_i)$ for each class with respect to $x$.\n",
    "   3. At last, we compare the joint probability of $x$ and $w_i$ instead of conditional probability.\n",
    "   \n",
    "   ---\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing the Classification Error Probability\n",
    "\n",
    "**The Error probability**\n",
    "<img src=\"bayes1.JPG\" width=\"60%\" />\n",
    "The shaded area in about graph is the error probability of a two classes classification task.\n",
    "\n",
    "**Conclusion**: Bayesian classifier is optimal with respect to minimizing the classification error probability.\n",
    "\n",
    "**Proof**\n",
    "Let $R_1$ be the region of the feature space we classified as $w_1$.\n",
    "\n",
    "$R_2$ be the region corresponding to $w_2$.\n",
    "\n",
    "So the Error probability is the sum of two join probabilities:\n",
    "\n",
    "$$ P_e = P(x\\in R_2,w_1) + P(x \\in R_1,w_2)$$\n",
    "\n",
    "Using basic probability rules\n",
    "\n",
    "$$ P_e = P(x\\in R_2|w_1)P(w_1) + P(x\\in R_2|w_2)P(w_2) \\\\\n",
    "       = P(w_1)\\underset{R_2}{\\int}p(x|w_1)dx + P(w_2)\\underset{R_1}{\\int}p(x|w_2)dx\n",
    "$$\n",
    "\n",
    "Using bayes rule:\n",
    "\n",
    "$$ P_e = \\underset{R_2}\\int{P(w_1|x)P(x)dx} + \\underset{R_1}\\int P(w_2|x)p(x)dx \\quad (A)$$ \n",
    "\n",
    "And we know $R_1,R_2$ covers all the vector space.\n",
    "\n",
    "$$\\underset{R_1}\\int{P(w_1|x)p(x)dx} + \\underset{R_2}\\int{P(w_1|x)p(x)dx} = P(w_1) \\quad (B)$$\n",
    "\n",
    "Combine (A) and (B) we have:\n",
    "\n",
    "$$ P_e = P(w_1) - \\underset{R_1}\\int{(P(w_1|x) - P(w_2|x))p(x)dx}$$\n",
    "\n",
    "So if we want to minimize the $P_e$, we need to choose thepartitionaing region $R_1,R_2$ so that:\n",
    "$$ R_1: P(w_1|x) > P(w_2|x)\\\\\n",
    "   R_2: P(w_2|x) > P(w_1|x)$$\n",
    "   \n",
    "***Generalizations to multiclass case***\n",
    "\n",
    " $x$ is assigned to $w_i$ if $$P(w_i|x) \\gt P(w_j|x)\\quad  \\forall j\\ne i$$\n",
    " \n",
    " ***my proof***\n",
    " \n",
    " \n",
    " \n",
    " $$\n",
    " \\begin{eqnarray}\n",
    "      P_e &=& 1 \\quad-\\quad \\Sigma{P(x\\in R_i,w_i)}\\\\\n",
    "          &=& 1 \\quad-\\quad \\Sigma{p(x\\in R_i|w_i)P(w_i)}\\\\\n",
    "          &=& 1 \\quad-\\quad \\Sigma{\\underset{R_i}{\\int}p(x|w_i)P(w_i)dx}\\\\\n",
    "          &=& 1 \\quad-\\quad \\Sigma{\\underset{R_i}{\\int}P(w_i|x)p(x)dx}\n",
    " \\end{eqnarray}\n",
    " $$\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
